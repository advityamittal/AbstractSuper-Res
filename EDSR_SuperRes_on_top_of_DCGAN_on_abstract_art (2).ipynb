{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLLTvfIN09FQ"
      },
      "source": [
        "# DCGAN to generate abstract art\n",
        "\n",
        "**Author:** [Advitya Mittal] <br>\n",
        "**Date created:** 2023/07/30<br>\n",
        "**Description:** Implementing 4x and 16x super resolution EDSR on top of DCGAN trained using `fit()` by overriding `train_step` on images of Abstract art.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5x9f5-iv09FT"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k3UdBP9J09FU"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import gdown\n",
        "from zipfile import ZipFile\n",
        "from PIL import Image\n",
        "\n",
        "import numpy as np\n",
        "from tensorflow.keras.layers import Input, Conv2D, Add, BatchNormalization, PReLU, LeakyReLU, UpSampling2D, Concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.initializers import Initializer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJn_mqfc09FV"
      },
      "source": [
        "## Prepare Abstract Art data\n",
        "\n",
        "We'll use face images from an Abstract art dataset, resized to 64x64 for efficient computation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3NRNoeW09FV"
      },
      "outputs": [],
      "source": [
        "if os.path.exists(\"art\"):\n",
        "    # Remove the \"art\" directory if it already exists\n",
        "    import shutil\n",
        "    shutil.rmtree(\"art\")\n",
        "\n",
        "os.makedirs(\"art\")\n",
        "\n",
        "url = \"https://drive.google.com/uc?id=1zKQZ8lFYKZxpDUV1I4dq_eQR-f5L2yLB\"\n",
        "output = \"art/data.zip\"\n",
        "gdown.download(url, output, quiet=True)\n",
        "\n",
        "with ZipFile(\"art/data.zip\", \"r\") as zipobj:\n",
        "    zipobj.extractall(\"art\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmmoaxAh09FW"
      },
      "source": [
        "Create a dataset from our folder, and rescale the images to the [0-1] range:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQ0HlE9C09FW",
        "outputId": "873d9a87-43d3-4b7a-9c6d-b62c559c810a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2872 files belonging to 1 classes.\n"
          ]
        }
      ],
      "source": [
        "dataset = keras.utils.image_dataset_from_directory(\n",
        "    \"art\", label_mode=None, image_size=(64, 64), batch_size=32  # changing from 64,64\n",
        ")\n",
        "dataset = dataset.map(lambda x: x / 255.0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XC8QgXsv09FX"
      },
      "source": [
        "Let's display a sample image:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "11rM35ey09FX",
        "outputId": "265089b7-330d-4fa3-b14f-003a014e8f32"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "\n",
        "for x in dataset:\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow((x.numpy() * 255).astype(\"int32\")[0])\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9VuUx-G09FY"
      },
      "source": [
        "## Create the discriminator\n",
        "\n",
        "It maps a 64x64 image to a binary classification score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJmwXj2509FY",
        "outputId": "e23ba4f3-db2d-4fa4-de35-5a2178007eb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"discriminator\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 32, 32, 64)        3136      \n",
            "                                                                 \n",
            " leaky_re_lu (LeakyReLU)     (None, 32, 32, 64)        0         \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 16, 16, 128)       131200    \n",
            "                                                                 \n",
            " leaky_re_lu_1 (LeakyReLU)   (None, 16, 16, 128)       0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 8, 8, 128)         262272    \n",
            "                                                                 \n",
            " leaky_re_lu_2 (LeakyReLU)   (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 8192)              0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 8192)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 8193      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 404,801\n",
            "Trainable params: 404,801\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "discriminator = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(64, 64, 3)),\n",
        "        layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0.2),\n",
        "        layers.Dense(1, activation=\"sigmoid\"),\n",
        "    ],\n",
        "    name=\"discriminator\",\n",
        ")\n",
        "discriminator.summary()\n",
        "# discriminator = keras.Sequential(\n",
        "#     [\n",
        "#         keras.Input(shape=(128, 128, 3)),\n",
        "#         layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\"),\n",
        "#         layers.LeakyReLU(alpha=0.2),\n",
        "#         layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "#         layers.LeakyReLU(alpha=0.2),\n",
        "#         layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "#         layers.LeakyReLU(alpha=0.2),\n",
        "#         layers.Flatten(),\n",
        "#         layers.Dropout(0.2),\n",
        "#         layers.Dense(1, activation=\"sigmoid\"),\n",
        "#     ],\n",
        "#     name=\"discriminator\",\n",
        "# )\n",
        "# discriminator.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2OOHFAA09FY"
      },
      "source": [
        "## Create the generator\n",
        "\n",
        "It mirrors the discriminator, replacing `Conv2D` layers with `Conv2DTranspose` layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3JbS9Og09FY",
        "outputId": "7d0b16c6-b176-4460-bb86-12399d3f78c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"generator\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_1 (Dense)             (None, 8192)              1056768   \n",
            "                                                                 \n",
            " reshape (Reshape)           (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_transpose (Conv2DTra  (None, 16, 16, 128)      262272    \n",
            " nspose)                                                         \n",
            "                                                                 \n",
            " leaky_re_lu_3 (LeakyReLU)   (None, 16, 16, 128)       0         \n",
            "                                                                 \n",
            " conv2d_transpose_1 (Conv2DT  (None, 32, 32, 256)      524544    \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " leaky_re_lu_4 (LeakyReLU)   (None, 32, 32, 256)       0         \n",
            "                                                                 \n",
            " conv2d_transpose_2 (Conv2DT  (None, 64, 64, 512)      2097664   \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " leaky_re_lu_5 (LeakyReLU)   (None, 64, 64, 512)       0         \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 64, 64, 3)         38403     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,979,651\n",
            "Trainable params: 3,979,651\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "latent_dim = 128\n",
        "generator = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(latent_dim,)),\n",
        "        layers.Dense(8 * 8 * 128),\n",
        "        layers.Reshape((8, 8, 128)),\n",
        "        layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2DTranspose(512, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2D(3, kernel_size=5, padding=\"same\", activation=\"sigmoid\"),\n",
        "    ],\n",
        "    name=\"generator\",\n",
        ")\n",
        "generator.summary()\n",
        "# latent_dim = 128\n",
        "\n",
        "# generator = keras.Sequential(\n",
        "#     [\n",
        "#         keras.Input(shape=(latent_dim,)),\n",
        "#         layers.Dense(8 * 8 * 128),\n",
        "#         layers.Reshape((8, 8, 128)),\n",
        "#         layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "#         layers.LeakyReLU(alpha=0.2),\n",
        "#         layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding=\"same\"),\n",
        "#         layers.LeakyReLU(alpha=0.2),\n",
        "#         layers.Conv2DTranspose(512, kernel_size=4, strides=2, padding=\"same\"),\n",
        "#         layers.LeakyReLU(alpha=0.2),\n",
        "#         layers.Conv2D(3, kernel_size=5, padding=\"same\", activation=\"sigmoid\"),\n",
        "#     ],\n",
        "#     name=\"generator\",\n",
        "# )\n",
        "# generator.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "028bRoqI09FY"
      },
      "source": [
        "## Override `train_step`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xVG6-hW609FY"
      },
      "outputs": [],
      "source": [
        "\n",
        "class GAN(keras.Model):\n",
        "    def __init__(self, discriminator, generator, latent_dim):\n",
        "        super().__init__()\n",
        "        self.discriminator = discriminator\n",
        "        self.generator = generator\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
        "        super().compile()\n",
        "        self.d_optimizer = d_optimizer\n",
        "        self.g_optimizer = g_optimizer\n",
        "        self.loss_fn = loss_fn\n",
        "        self.d_loss_metric = keras.metrics.Mean(name=\"d_loss\")\n",
        "        self.g_loss_metric = keras.metrics.Mean(name=\"g_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.d_loss_metric, self.g_loss_metric]\n",
        "\n",
        "    def train_step(self, real_images):\n",
        "        # Sample random points in the latent space\n",
        "        batch_size = tf.shape(real_images)[0]\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "\n",
        "        # Decode them to fake images\n",
        "        generated_images = self.generator(random_latent_vectors)\n",
        "\n",
        "        # # Combine them with real images\n",
        "        # # Upscale the generated images to match the size of real images\n",
        "        # generated_images = tf.image.resize(generated_images, (128, 128))\n",
        "\n",
        "        # Combine them with real images\n",
        "        combined_images = tf.concat([generated_images, real_images], axis=0)\n",
        "\n",
        "        # Assemble labels discriminating real from fake images\n",
        "        labels = tf.concat(\n",
        "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
        "        )\n",
        "        # Add random noise to the labels - important trick!\n",
        "        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n",
        "\n",
        "        # Train the discriminator\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(combined_images)\n",
        "            d_loss = self.loss_fn(labels, predictions)\n",
        "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
        "        self.d_optimizer.apply_gradients(\n",
        "            zip(grads, self.discriminator.trainable_weights)\n",
        "        )\n",
        "\n",
        "        # Sample random points in the latent space\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "\n",
        "        # Assemble labels that say \"all real images\"\n",
        "        misleading_labels = tf.zeros((batch_size, 1))\n",
        "\n",
        "        # Train the generator (note that we should *not* update the weights\n",
        "        # of the discriminator)!\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(self.generator(random_latent_vectors))\n",
        "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
        "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
        "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
        "\n",
        "        # Update metrics\n",
        "        self.d_loss_metric.update_state(d_loss)\n",
        "        self.g_loss_metric.update_state(g_loss)\n",
        "        return {\n",
        "            \"d_loss\": self.d_loss_metric.result(),\n",
        "            \"g_loss\": self.g_loss_metric.result(),\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDDaYCAJ09FZ"
      },
      "source": [
        "## Create a callback that periodically saves generated images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-uNOwsX09FZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "# class GANMonitor(keras.callbacks.Callback):\n",
        "#     def __init__(self, num_img=3, latent_dim=128):\n",
        "#         self.num_img = num_img\n",
        "#         self.latent_dim = latent_dim\n",
        "#         self.save_dir = \"/content/sample_data/saved\"\n",
        "\n",
        "#     def on_epoch_end(self, epoch, logs=None):\n",
        "#         random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n",
        "#         generated_images = self.model.generator(random_latent_vectors)\n",
        "#         generated_images *= 255\n",
        "#         generated_images.numpy()\n",
        "#         for i in range(self.num_img):\n",
        "#             img = keras.utils.array_to_img(generated_images[i])\n",
        "#             #img.save(\"generated_img_%03d_%d.png\" % (epoch, i))\n",
        "#             img.save(os.path.join(self.save_dir, \"generated_img_%03d_%d.png\" % (epoch, i)))\n",
        "\n",
        "class GANMonitor(keras.callbacks.Callback):\n",
        "    def __init__(self, num_img=3, latent_dim=128):\n",
        "        self.num_img = num_img\n",
        "        self.latent_dim = latent_dim\n",
        "        self.save_dir = \"/content/sample_data/saved\"\n",
        "        self.save_epochs = [1, 3, 7, 9, 10, 15, 25, 40, 60, 80, 100]  # Define the epochs to save images\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if (epoch + 1) in self.save_epochs:  # Check if the current epoch is in the save_epochs list\n",
        "            random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n",
        "            generated_images = self.model.generator(random_latent_vectors)\n",
        "            generated_images *= 255\n",
        "            generated_images.numpy()\n",
        "            for i in range(self.num_img):\n",
        "                img = keras.utils.array_to_img(generated_images[i])\n",
        "                img.save(os.path.join(self.save_dir, \"generated_img_%03d_%d.png\" % (epoch + 1, i)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuNISIZn09Fa"
      },
      "source": [
        "## Train the end-to-end model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uX3y0FJo09Fa",
        "outputId": "e3e77f13-1910-40cf-88a9-375654611787"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "90/90 [==============================] - 47s 440ms/step - d_loss: 0.6161 - g_loss: 3.4331\n",
            "Epoch 2/10\n",
            "90/90 [==============================] - 44s 457ms/step - d_loss: 0.5863 - g_loss: 2.6660\n",
            "Epoch 3/10\n",
            "90/90 [==============================] - 44s 440ms/step - d_loss: -0.1744 - g_loss: 59.5434\n",
            "Epoch 4/10\n",
            "90/90 [==============================] - 43s 435ms/step - d_loss: -285.3616 - g_loss: 53948.1016\n",
            "Epoch 5/10\n",
            "90/90 [==============================] - 43s 442ms/step - d_loss: -921.5095 - g_loss: 150163.9531\n",
            "Epoch 6/10\n",
            "90/90 [==============================] - 43s 438ms/step - d_loss: -35183.7930 - g_loss: 4007577.0000\n",
            "Epoch 7/10\n",
            "90/90 [==============================] - 44s 446ms/step - d_loss: -122769.0234 - g_loss: 19043128.0000\n",
            "Epoch 8/10\n",
            "90/90 [==============================] - 43s 440ms/step - d_loss: -802985.8750 - g_loss: 73966248.0000\n",
            "Epoch 9/10\n",
            "90/90 [==============================] - 49s 471ms/step - d_loss: -100648.9453 - g_loss: 15961599.0000\n",
            "Epoch 10/10\n",
            "90/90 [==============================] - 47s 483ms/step - d_loss: 38844.1797 - g_loss: 886068.6250\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7edcc87be4d0>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "epochs = 10  # In practice, use ~100 epochs\n",
        "\n",
        "gan = GAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim)\n",
        "gan.compile(\n",
        "    d_optimizer=keras.optimizers.Adam(learning_rate=0.0008),  # changed both from 0.0001\n",
        "    g_optimizer=keras.optimizers.Adam(learning_rate=0.0008),\n",
        "    loss_fn=keras.losses.BinaryCrossentropy(),\n",
        ")\n",
        "\n",
        "gan.fit(\n",
        "    dataset, epochs=epochs, callbacks=[GANMonitor(num_img=10, latent_dim=latent_dim)]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def srgan_generator(scale_factor=8, num_filters=64, num_res_blocks=16):\n",
        "#     def res_block(inputs, num_filters):\n",
        "#         x = Conv2D(num_filters, 3, padding='same')(inputs)\n",
        "#         x = BatchNormalization()(x)\n",
        "#         x = PReLU(alpha_initializer='zeros')(x)\n",
        "#         x = Conv2D(num_filters, 3, padding='same')(x)\n",
        "#         x = BatchNormalization()(x)\n",
        "#         x = Add()([x, inputs])\n",
        "#         return x\n",
        "\n",
        "#     inputs = Input(shape=(None, None, 3))\n",
        "#     x = Conv2D(num_filters, 9, padding='same')(inputs)\n",
        "#     x = PReLU(alpha_initializer='zeros')(x)\n",
        "\n",
        "#     for _ in range(num_res_blocks):\n",
        "#         x = res_block(x, num_filters)\n",
        "\n",
        "#     x = Conv2D(num_filters, 3, padding='same')(x)\n",
        "#     x = BatchNormalization()(x)\n",
        "#     x = Add()([x, inputs])\n",
        "\n",
        "#     # Upsampling\n",
        "#     for _ in range(int(np.log2(scale_factor))):\n",
        "#         x = UpSampling2D(interpolation='bilinear')(x)\n",
        "#         x = Conv2D(num_filters, 3, padding='same')(x)\n",
        "#         x = PReLU(alpha_initializer='zeros')(x)\n",
        "\n",
        "#     x = Conv2D(3, 9, padding='same', activation='sigmoid')(x)\n",
        "\n",
        "#     model = Model(inputs, x)\n",
        "#     return model\n",
        "\n",
        "\n",
        "def edsr(scale_factor=4, num_filters=64, num_res_blocks=16):\n",
        "    def res_block(inputs, num_filters):\n",
        "        x = Conv2D(num_filters, 3, padding='same', activation='relu')(inputs)\n",
        "        x = Conv2D(num_filters, 3, padding='same')(x)\n",
        "        x = Add()([x, inputs])\n",
        "        return x\n",
        "\n",
        "    inputs = Input(shape=(None, None, 3))\n",
        "    x = Conv2D(num_filters, 3, padding='same', activation='relu')(inputs)\n",
        "\n",
        "    for _ in range(num_res_blocks):\n",
        "        x = res_block(x, num_filters)\n",
        "\n",
        "    x = Conv2D(3 * scale_factor ** 2, 3, padding='same')(x)\n",
        "    x = tf.nn.depth_to_space(x, scale_factor)\n",
        "\n",
        "    model = Model(inputs, x)\n",
        "    return model\n",
        "\n",
        "# def edsr(scale_factor=16, num_filters=64, num_res_blocks=32):\n",
        "#     def res_block(inputs, num_filters):\n",
        "#         x = Conv2D(num_filters, 3, padding='same', activation='relu')(inputs)\n",
        "#         x = Conv2D(num_filters, 3, padding='same')(x)\n",
        "#         x = Add()([x, inputs])\n",
        "#         return x\n",
        "\n",
        "#     inputs = Input(shape=(None, None, 3))\n",
        "#     x = Conv2D(num_filters, 3, padding='same', activation='relu')(inputs)\n",
        "\n",
        "#     for _ in range(num_res_blocks):\n",
        "#         x = res_block(x, num_filters)\n",
        "\n",
        "#     x = Conv2D(3 * scale_factor ** 2, 3, padding='same')(x)\n",
        "#     x = tf.nn.depth_to_space(x, scale_factor)\n",
        "\n",
        "#     model = Model(inputs, x)\n",
        "#     return model\n",
        "\n",
        "\n",
        "# Data preparation\n",
        "def load_images(path, target_size=(64, 64)):\n",
        "    image_files = [file for file in os.listdir(path) if file.endswith(('.png', '.jpg', '.jpeg'))]\n",
        "    images = []\n",
        "    for image_file in image_files:\n",
        "        image_path = os.path.join(path, image_file)\n",
        "        try:\n",
        "            image = Image.open(image_path)\n",
        "            image = image.resize(target_size)  # Resize to target_size for low-res images\n",
        "            image = np.array(image)\n",
        "            images.append(image)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {image_path}: {e}\")\n",
        "    return np.array(images)\n",
        "\n",
        "def normalize_images(images):\n",
        "    return images.astype('float32') / 255.0"
      ],
      "metadata": {
        "id": "br2vma46du1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load training data\n",
        "train_lr_path = '/content/sample_data/train_lr'\n",
        "train_hr_path = '/content/sample_data/train_hr'\n",
        "train_lr_images = load_images(train_lr_path, target_size=(64, 64))\n",
        "train_hr_images = load_images(train_hr_path, target_size=(256, 256)) # 256,256\n",
        "print(\"Shape of train_lr_images:\", train_lr_images.shape)\n",
        "print(\"Shape of train_hr_images:\", train_hr_images.shape)\n",
        "# Normalize images\n",
        "train_lr_images = normalize_images(train_lr_images)\n",
        "train_hr_images = normalize_images(train_hr_images)\n",
        "\n",
        "# Create and compile the model\n",
        "model = edsr()\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
        "# Train the model\n",
        "model.fit(train_lr_images, train_hr_images, epochs=20, batch_size=16)\n",
        "\n",
        "# # Create and compile the model\n",
        "# model = edsr(scale_factor=16)\n",
        "# model.compile(optimizer=Adam(learning_rate=0.0005), loss='mse')\n",
        "# # Train the model\n",
        "# model.fit(train_lr_images, train_hr_images, epochs=300, batch_size=16)\n",
        "\n",
        "# Load the saved images\n",
        "saved_images_path = '/content/sample_data/saved'\n",
        "saved_images = load_images(saved_images_path)\n",
        "saved_images = normalize_images(saved_images)\n",
        "\n",
        "# Perform super-resolution\n",
        "super_res_images = model.predict(saved_images)\n",
        "\n",
        "# Save the super-resolved images\n",
        "saved_super_res_path = '/content/sample_data/savedSuperRes'\n",
        "os.makedirs(saved_super_res_path, exist_ok=True)\n",
        "for i, super_res_image in enumerate(super_res_images):\n",
        "    filename = f'super_res_{i}.png'\n",
        "    save_path = os.path.join(saved_super_res_path, filename)\n",
        "    tf.keras.preprocessing.image.save_img(save_path, super_res_image)"
      ],
      "metadata": {
        "id": "V8sblWfgeEdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNa_9SYE09Fb"
      },
      "source": [
        "Some of the last generated images around epoch 30\n",
        "(results keep improving after that):\n",
        "\n",
        "![results](https://i.imgur.com/ove13lO.png)\n",
        "\n",
        "<!-- https://i.imgur.com/h5MtQZ7l.png -->"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}